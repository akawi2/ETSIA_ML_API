services:
  # PostgreSQL pour les métriques et le monitoring
  postgres:
    image: postgres:16-alpine
    container_name: etsia-postgres
    ports:
      - "5432:5432"
    environment:
      POSTGRES_USER: etsia
      POSTGRES_PASSWORD: etsia_secure_password
      POSTGRES_DB: etsia_metrics
    volumes:
      - postgres-data:/var/lib/postgresql/data
      - ./scripts/init_db.sql:/docker-entrypoint-initdb.d/init_db.sql
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U etsia -d etsia_metrics"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s
    networks:
      - api-network

  # Ollama service for LLM inference (Qwen 2.5 1.5B + Llama 3.2 3B)
  ollama:
    image: ollama/ollama:latest
    container_name: ollama-server
    ports:
      - "11434:11434"
    volumes:
      - ollama-data:/root/.ollama
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    networks:
      - api-network
    # Optional: Uncomment for GPU support
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: 1
    #           capabilities: [gpu]

  # API avec support CPU uniquement (production légère)
  api:
    build: 
      context: .
      dockerfile: Dockerfile
      args:
        BASE_IMAGE: python:3.11-slim
    image: etsia-ml-api:cpu
    container_name: etsia-ml-api-cpu
    ports:
      - "8000:8000"
    env_file:
      - .env
    environment:
      - LOG_LEVEL=info
      - API_ENV=production
      - OLLAMA_BASE_URL=http://ollama:11434
    restart: unless-stopped
    depends_on:
      ollama:
        condition: service_healthy
      postgres:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 30s
      retries: 3
      start_period: 60s
    networks:
      - api-network
    volumes:
      - model_cache:/app/.cache
      - huggingface_cache:/app/.cache/huggingface
    deploy:
      resources:
        limits:
          memory: 8G
        reservations:
          memory: 2G


  # API avec support GPU (performance optimale)
  api-gpu:
    build: 
      context: .
      dockerfile: Dockerfile
      args:
        BASE_IMAGE: nvidia/cuda:12.1-runtime-ubuntu22.04
    image: etsia-ml-api:gpu
    container_name: etsia-ml-api-gpu
    ports:
      - "8001:8000"
    env_file:
      - .env
    environment:
      - LOG_LEVEL=info
      - API_ENV=production
      - CUDA_VISIBLE_DEVICES=0
      - OLLAMA_BASE_URL=http://ollama:11434
    restart: unless-stopped
    depends_on:
      ollama:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 30s
      retries: 3
      start_period: 120s
    networks:
      - api-network
    volumes:
      - model_cache:/app/.cache
      - huggingface_cache:/app/.cache/huggingface
    deploy:
      resources:
        limits:
          memory: 8G
        reservations:
          memory: 4G
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    profiles:
      - gpu

volumes:
  postgres-data:
    driver: local
  ollama-data:
    driver: local
  model_cache:
    driver: local
  huggingface_cache:
    driver: local

networks:
  api-network:
    driver: bridge
    name: etsia-ml-network
