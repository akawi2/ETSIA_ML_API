# üöÄ START HERE - D√©marrage Complet du Projet

Guide de d√©marrage complet pour lancer l'API avec tous les mod√®les (texte + images).

---

## üìã Checklist Avant de Commencer

- [ ] Python 3.8+ install√©
- [ ] Git install√©
- [ ] Cl√©s API configur√©es (OpenAI/Anthropic) OU Ollama install√©
- [ ] ~5 GB d'espace disque (pour les mod√®les d'images)

---

## ‚ö° Installation Rapide

### 1. Cloner et Configurer

```bash
# Cloner le projet
git clone <votre-repo>
cd ETSIA_ML_API

# Cr√©er un environnement virtuel
python -m venv venv

# Activer l'environnement (Windows)
.\venv\Scripts\activate

# Activer l'environnement (Linux/Mac)
source venv/bin/activate
```

### 2. Installer les D√©pendances

```bash
# D√©pendances principales
pip install -r requirements.txt

# D√©pendances pour l'analyse d'images
pip install -r app/services/sensitive_image_caption/requirements.txt
```

**Packages principaux install√©s :**
- FastAPI, Uvicorn (API)
- Transformers, Torch (ML)
- OpenAI, Anthropic (LLM)
- Pillow (Images)

### 3. Configurer les Variables d'Environnement

```bash
# Copier le fichier exemple
copy .env.example .env

# √âditer .env avec vos configurations
notepad .env
```

**Configuration minimale pour LLM local (gratuit) :**
```env
LLM_PROVIDER=local
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3.2
```

**OU avec GPT (payant mais rapide) :**
```env
LLM_PROVIDER=gpt
OPENAI_API_KEY=sk-votre-cle-ici
OPENAI_MODEL=gpt-4o-mini
```

---

## üé¨ Lancer l'API

### M√©thode 1 : D√©veloppement (Recommand√©)

```bash
# Activer l'environnement virtuel
.\venv\Scripts\activate

# Lancer l'API en mode d√©veloppement
uvicorn app.main:app --reload --port 8000
```

**Vous devriez voir :**
```
======================================================================
Depression Detection API v1.0.0
Architecture Multi-Mod√®les
======================================================================

üì¶ Enregistrement des mod√®les...
----------------------------------------------------------------------
‚úì yansnet-llm v1.0.0 by √âquipe YANSNET [D√âFAUT]
‚úì Mod√®le de d√©tection de contenu sensible (images) enregistr√©
‚úì sensitive-image-caption v1.0.0 by Votre √âquipe
----------------------------------------------------------------------
‚úì 2 mod√®le(s) enregistr√©(s)

======================================================================
‚úì API d√©marr√©e avec succ√®s!
üìö Documentation: http://localhost:8000/docs
üìã Mod√®les disponibles: http://localhost:8000/api/v1/models
======================================================================
```

### M√©thode 2 : Production

```bash
uvicorn app.main:app --host 0.0.0.0 --port 8000 --workers 4
```

### M√©thode 3 : Docker

```bash
# Construire l'image
docker build -t etsia-ml-api .

# Lancer le conteneur
docker run -p 8000:8000 --env-file .env etsia-ml-api
```

---

## üß™ Tester l'Installation

### Test 1 : Health Check

```bash
curl http://localhost:8000/health
```

**R√©sultat attendu :**
```json
{
  "status": "healthy",
  "version": "1.0.0",
  "models": {
    "total": 2,
    "available": ["yansnet-llm", "sensitive-image-caption"]
  }
}
```

### Test 2 : Lister les Mod√®les

```bash
curl http://localhost:8000/api/v1/models
```

**R√©sultat attendu :**
```json
{
  "models": {
    "yansnet-llm": {...},
    "sensitive-image-caption": {...}
  },
  "total": 2
}
```

### Test 3 : Analyse de Texte

```bash
curl -X POST http://localhost:8000/api/v1/predict \
  -H "Content-Type: application/json" \
  -d "{\"text\": \"I feel so sad and hopeless\", \"include_reasoning\": true}"
```

### Test 4 : Analyse d'Image

```bash
# Cr√©er une image de test ou utiliser une vraie image
curl -X POST http://localhost:8000/api/v1/predict-image \
  -F "image=@path/to/your/image.jpg"
```

### Test 5 : Documentation Interactive

**Ouvrir dans votre navigateur :**
```
http://localhost:8000/docs
```

Vous pouvez tester tous les endpoints directement depuis Swagger UI !

---

## üìä Tests Python

### Test avec le Script de D√©mo

```bash
# Test du mod√®le d'images
python demo_image_analysis.py

# Test avec vos propres images
python demo_image_analysis.py image1.jpg image2.jpg
```

### Tests Unitaires

```bash
# Tous les tests
pytest tests/ -v

# Tests du mod√®le texte uniquement
pytest tests/test_api.py -v

# Tests du mod√®le images uniquement
pytest tests/test_image_model.py -v

# Tests avec couverture
pytest tests/ --cov=app --cov-report=html
```

---

## üêõ D√©pannage

### Probl√®me : "Mod√®le non enregistr√©"

**Sympt√¥me :** L'API d√©marre mais un mod√®le manque

**Solution :**
```bash
# V√©rifier les logs au d√©marrage
# Chercher : ‚úó Erreur lors de l'enregistrement...

# R√©installer les d√©pendances manquantes
pip install transformers torch Pillow sentencepiece --upgrade
```

### Probl√®me : "Out of Memory"

**Sympt√¥me :** Erreur lors du chargement du mod√®le d'images

**Solution 1 - Utiliser CPU :**
```bash
# Windows
set CUDA_VISIBLE_DEVICES=
uvicorn app.main:app --reload

# Linux/Mac
export CUDA_VISIBLE_DEVICES=""
uvicorn app.main:app --reload
```

**Solution 2 - Augmenter la m√©moire :**
- Fermer les autres applications
- Red√©marrer le syst√®me

### Probl√®me : "Ollama not found"

**Sympt√¥me :** Erreur avec `LLM_PROVIDER=local`

**Solution :**
```bash
# 1. Installer Ollama
# Windows: https://ollama.ai/download
# Linux: curl -fsSL https://ollama.ai/install.sh | sh

# 2. T√©l√©charger un mod√®le
ollama pull llama3.2

# 3. Lancer le serveur
ollama serve

# 4. V√©rifier
curl http://localhost:11434/api/tags
```

### Probl√®me : "Connection refused"

**Sympt√¥me :** Impossible de se connecter √† l'API

**Solutions :**
- V√©rifier que l'API est bien lanc√©e
- V√©rifier le port (d√©faut: 8000)
- V√©rifier le firewall
- Essayer : `http://127.0.0.1:8000` au lieu de `localhost`

### Probl√®me : Lenteur Excessive (Premi√®re Fois)

**Explication :** Les mod√®les se t√©l√©chargent automatiquement (~2-3 GB)

**Progression :**
```
Downloading microsoft/git-large-textcaps...
Downloading Helsinki-NLP/opus-mt-en-fr...
```

**Solution :** Patienter ou pr√©-t√©l√©charger :
```bash
python -c "from transformers import GitProcessor, GitForCausalLM; \
  GitProcessor.from_pretrained('microsoft/git-large-textcaps'); \
  GitForCausalLM.from_pretrained('microsoft/git-large-textcaps')"
```

---

## üìö Documentation

### Guides Principaux

| Document | Description |
|----------|-------------|
| **README.md** | Vue d'ensemble du projet |
| **QUICK_START_IMAGE.md** | D√©marrage rapide analyse d'images |
| **docs/IMAGE_ANALYSIS_GUIDE.md** | Guide complet analyse d'images |
| **docs/ADD_YOUR_MODEL.md** | Ajouter votre propre mod√®le |
| **docs/API_CONTRACT.md** | Contrat API d√©taill√© |
| **CHANGELOG.md** | Historique des modifications |

### Acc√®s Rapide

```bash
# Documentation interactive
http://localhost:8000/docs

# Alternative (ReDoc)
http://localhost:8000/redoc

# Health check
http://localhost:8000/health

# Liste des mod√®les
http://localhost:8000/api/v1/models
```

---

## üéØ Prochaines √âtapes

### 1. Tester avec Vos Donn√©es

```python
import requests

# Analyser un texte
response = requests.post(
    "http://localhost:8000/api/v1/predict",
    json={"text": "Votre texte ici"}
)
print(response.json())

# Analyser une image
with open("votre_image.jpg", "rb") as f:
    response = requests.post(
        "http://localhost:8000/api/v1/predict-image",
        files={"image": f}
    )
print(response.json())
```

### 2. Personnaliser le Mod√®le d'Images

```python
# √âditer app/services/sensitive_image_caption/sensitive_image_caption_model.py

# Ajouter vos propres mots-cl√©s sensibles
SENSITIVE_KEYWORDS.update({
    'votre_mot_cle',
    'autre_mot'
})
```

### 3. Ajouter Votre Propre Mod√®le

Suivre le guide : **docs/ADD_YOUR_MODEL.md**

### 4. D√©ployer en Production

Voir : **docs/DEPLOYMENT.md**

---

## üí° Conseils Pro

### Optimisation Performance

```bash
# Utiliser GPU si disponible
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118

# V√©rifier
python -c "import torch; print(torch.cuda.is_available())"
```

### Monitoring

```bash
# Logs en temps r√©el
tail -f app.log

# M√©triques
curl http://localhost:8000/health
```

### S√©curit√©

1. **Ne jamais commit .env** : Contient vos cl√©s API
2. **Utiliser HTTPS** en production
3. **Activer l'authentification** si besoin
4. **Rate limiting** pour √©viter les abus

---

## ‚úÖ Checklist de D√©marrage R√©ussi

V√©rifiez que tout fonctionne :

- [ ] L'API d√©marre sans erreur
- [ ] 2 mod√®les sont enregistr√©s (yansnet-llm, sensitive-image-caption)
- [ ] `/health` retourne "healthy"
- [ ] `/api/v1/models` liste les 2 mod√®les
- [ ] `/api/v1/predict` fonctionne avec un texte
- [ ] `/api/v1/predict-image` fonctionne avec une image
- [ ] `/docs` affiche la documentation Swagger
- [ ] Les tests passent : `pytest tests/`

---

## üÜò Support

### Ressources

- **Documentation** : Dossier `docs/`
- **Exemples** : Dossier `tests/`
- **D√©mo** : `demo_image_analysis.py`

### Commandes Utiles

```bash
# R√©installer tout
pip install -r requirements.txt --force-reinstall

# Nettoyer le cache
pip cache purge

# V√©rifier les versions
pip list | grep -E "fastapi|transformers|torch"

# Logs d√©taill√©s
uvicorn app.main:app --log-level debug
```

---

## üéâ F√©licitations !

Votre API multi-mod√®les est maintenant op√©rationnelle avec :
- ‚úÖ D√©tection de d√©pression dans les textes (LLM)
- ‚úÖ D√©tection de contenu sensible dans les images (Vision)
- ‚úÖ Architecture extensible pour ajouter d'autres mod√®les
- ‚úÖ Documentation compl√®te et tests

**Temps estim√© :** 10-20 minutes  
**Niveau :** D√©butant √† Interm√©diaire

---

**Bon d√©veloppement ! üöÄ**

*Si vous rencontrez des probl√®mes, consultez la section D√©pannage ou les guides dans `docs/`*
