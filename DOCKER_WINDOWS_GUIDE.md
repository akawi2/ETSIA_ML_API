# ğŸ³ Guide Docker pour Windows - ETSIA ML API

## ğŸ¯ **SystÃ¨me UnifiÃ©**

Le projet utilise maintenant **un seul Dockerfile** intelligent qui s'adapte automatiquement au CPU ou GPU selon vos besoins, avec **PostgreSQL** pour les mÃ©triques et **Ollama** pour les modÃ¨les LLM.

## ğŸ—ï¸ **Architecture des Services**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Docker Network                            â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  PostgreSQL  â”‚  â”‚    Ollama    â”‚  â”‚   API (FastAPI)  â”‚  â”‚
â”‚  â”‚  (MÃ©triques) â”‚  â”‚  (LLM/Qwen)  â”‚  â”‚   CPU ou GPU     â”‚  â”‚
â”‚  â”‚  Port: 5432  â”‚  â”‚  Port: 11434 â”‚  â”‚   Port: 8000     â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ **DÃ©ploiement Rapide**

### **Option 1: Docker Compose (RecommandÃ©)**
```powershell
# DÃ©marrer tous les services (PostgreSQL + Ollama + API)
docker-compose up -d

# TÃ©lÃ©charger les modÃ¨les Ollama
.\scripts\setup_ollama_models.bat
```

### **Option 2: Script PowerShell - CPU**
```powershell
.\docker-deploy.ps1 cpu
```
- **Port API**: 8000
- **Port PostgreSQL**: 5432
- **Port Ollama**: 11434
- **URL**: http://localhost:8000
- **Documentation**: http://localhost:8000/docs

### **Option 3: Script PowerShell - GPU**
```powershell
.\docker-deploy.ps1 gpu
```
- **Port API**: 8001  
- **URL**: http://localhost:8001
- **PrÃ©requis**: NVIDIA GPU + Docker GPU support

## ğŸ› ï¸ **Commandes Disponibles**

### **DÃ©ploiement**
```powershell
# CPU uniquement (dÃ©faut)
.\docker-deploy.ps1 cpu

# GPU avec CUDA
.\docker-deploy.ps1 gpu
```

### **Gestion**
```powershell
# ArrÃªter tous les services
.\docker-deploy.ps1 stop

# Voir les logs en temps rÃ©el
.\docker-deploy.ps1 logs

# VÃ©rifier la santÃ© de l'API
.\docker-deploy.ps1 health

# Nettoyage complet
.\docker-deploy.ps1 clean

# Aide
.\docker-deploy.ps1 help
```

## ğŸ”§ **Architecture Technique**

### **Dockerfile UnifiÃ©**
```dockerfile
# Utilise une image de base variable
ARG BASE_IMAGE=python:3.11-slim
FROM ${BASE_IMAGE}

# S'adapte automatiquement Ã  CPU ou GPU
```

### **Images GÃ©nÃ©rÃ©es**
- **CPU**: `etsia-ml-api:cpu` (basÃ©e sur python:3.11-slim)
- **GPU**: `etsia-ml-api:gpu` (basÃ©e sur nvidia/cuda:12.1-runtime)

### **Conteneurs CrÃ©Ã©s**
- **CPU**: `etsia-ml-api-cpu` sur port 8000
- **GPU**: `etsia-ml-api-gpu` sur port 8001

## ğŸ¯ **Avantages du Nouveau SystÃ¨me**

### âœ… **SimplicitÃ©**
- **1 seul Dockerfile** au lieu de 2
- **1 seul script** PowerShell optimisÃ© pour Windows
- **DÃ©tection automatique** des capacitÃ©s

### âœ… **FlexibilitÃ©**
- **Basculement facile** entre CPU et GPU
- **Coexistence possible** des deux modes
- **Configuration par variables d'environnement**

### âœ… **Maintenance**
- **Code unifiÃ©** plus facile Ã  maintenir
- **Moins de duplication**
- **Ã‰volution centralisÃ©e**

## ğŸ” **VÃ©rification du DÃ©ploiement**

### **Test Automatique**
Le script vÃ©rifie automatiquement la santÃ© aprÃ¨s dÃ©ploiement :
```
âœ… API CPU: Healthy
âœ… API GPU: Healthy
```

### **Test Manuel**
```powershell
# Tester l'endpoint de santÃ©
curl http://localhost:8000/health

# Tester une prÃ©diction
curl -X POST http://localhost:8000/api/v1/predict \
  -H "Content-Type: application/json" \
  -d '{"text": "Je me sens triste"}'
```

## ğŸš¨ **PrÃ©requis GPU**

Pour utiliser le mode GPU, assurez-vous d'avoir :

1. **GPU NVIDIA** compatible CUDA
2. **Pilotes NVIDIA** rÃ©cents
3. **NVIDIA Docker Runtime** installÃ©
4. **Docker Desktop** avec support GPU activÃ©

### **Installation NVIDIA Docker (Windows)**
```powershell
# Installer NVIDIA Container Toolkit
# Suivre: https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html
```

## ğŸ“Š **Comparaison des Performances**

| Mode | ModÃ¨les SupportÃ©s | Performance | MÃ©moire |
|------|------------------|-------------|---------|
| CPU  | Tous | Standard | ~2GB |
| GPU  | Tous + AccÃ©lÃ©ration | 3-10x plus rapide | ~4-8GB |

## ğŸ”§ **DÃ©pannage**

### **Erreur "Docker non installÃ©"**
```powershell
# Installer Docker Desktop pour Windows
# https://docs.docker.com/desktop/windows/install/
```

### **Erreur GPU non disponible**
```powershell
# VÃ©rifier le support GPU
docker run --rm --gpus all nvidia/cuda:12.1-base-ubuntu22.04 nvidia-smi
```

### **Port dÃ©jÃ  utilisÃ©**
```powershell
# ArrÃªter les services existants
.\docker-deploy.ps1 stop

# Ou changer le port dans le script
```

## ğŸ—„ï¸ **Services Docker**

### **PostgreSQL (MÃ©triques)**
```yaml
# Stockage des mÃ©triques de performance
Container: etsia-postgres
Port: 5432
Database: etsia_metrics
User: etsia
```

**AccÃ¨s Ã  la base de donnÃ©es:**
```powershell
# Via Docker
docker exec -it etsia-postgres psql -U etsia -d etsia_metrics

# RequÃªtes utiles
SELECT * FROM v_model_stats_24h;  # Stats 24h
SELECT * FROM v_active_alerts;     # Alertes actives
```

### **Ollama (LLM)**
```yaml
# ModÃ¨les LLM locaux
Container: ollama-server
Port: 11434
ModÃ¨les: qwen2.5:1.5b, llama3.2:3b, llama3.2:1b
```

**Gestion des modÃ¨les:**
```powershell
# Lister les modÃ¨les
docker exec ollama-server ollama list

# TÃ©lÃ©charger un modÃ¨le
docker exec ollama-server ollama pull qwen2.5:1.5b

# Tester un modÃ¨le
docker exec ollama-server ollama run qwen2.5:1.5b "Bonjour"
```

## ğŸ“Š **Endpoints de MÃ©triques**

| Endpoint | Description |
|----------|-------------|
| `GET /api/v1/metrics/health` | Health check PostgreSQL |
| `GET /api/v1/metrics/summary` | RÃ©sumÃ© global des mÃ©triques |
| `GET /api/v1/metrics/models` | Statistiques par modÃ¨le |
| `GET /api/v1/metrics/errors` | Erreurs rÃ©centes |
| `GET /api/v1/metrics/alerts` | Alertes actives |
| `GET /api/v1/metrics/prometheus` | Format Prometheus |

## ğŸ“š **Ressources**

- **Documentation API**: http://localhost:8000/docs
- **Health Check**: http://localhost:8000/health
- **ModÃ¨les disponibles**: http://localhost:8000/api/v1/models
- **MÃ©triques**: http://localhost:8000/api/v1/metrics/summary
- **DÃ©tection dÃ©pression**: http://localhost:8000/api/v1/depression/detect
- **Logs**: `docker-compose logs -f`

## ğŸ”§ **Variables d'Environnement**

CrÃ©ez un fichier `.env` basÃ© sur `.env.example`:

```bash
# Provider de dÃ©tection (camembert, qwen, xlm-roberta)
DETECTION_PROVIDER=qwen

# PostgreSQL
POSTGRES_HOST=postgres
POSTGRES_PORT=5432
POSTGRES_USER=etsia
POSTGRES_PASSWORD=etsia_secure_password
POSTGRES_DB=etsia_metrics

# Ollama
OLLAMA_BASE_URL=http://ollama:11434
QWEN_DETECTION_MODEL=qwen2.5:1.5b

# Monitoring
ENABLE_METRICS=true
LOG_LATENCY=true
```

---

**ğŸ‰ Votre API ETSIA ML est maintenant optimisÃ©e pour Windows avec PostgreSQL, Ollama et un systÃ¨me Docker unifiÃ© !**
